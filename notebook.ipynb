{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '',\n",
       " 'name': None,\n",
       " 'tool_calls': [{'id': '0yTgR5pLN',\n",
       "   'type': 'function',\n",
       "   'function': {'name': 'get_current_weather',\n",
       "    'arguments': '{\"location\": \"Paris\", \"unit\": \"celsius\"}'}}],\n",
       " 'tool_call_id': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:8000/v1/chat/completions\",\n",
    "    json={\n",
    "        \"model\": \"lit\",\n",
    "        \"stream\": False,  # You can stream chunked response by setting this True\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_current_weather\",\n",
    "                    \"description\": \"Get the current weather in a given location\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"unit\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}],\n",
    "    },\n",
    ")\n",
    "\n",
    "completion = response.json()\n",
    "completion[\"choices\"][0][\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from pathlib import Path\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "\n",
    "mistral_models_path = Path.home().joinpath(\"mistral_models\", \"7B-Instruct-v0.3\")\n",
    "mistral_tokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}\n",
    "]\n",
    "\n",
    "tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "completion_request = ChatCompletionRequest(messages=messages, tools=tools)\n",
    "text = mistral_tokenizer.encode_chat_completion(completion_request).text\n",
    "tokens = mistral_tokenizer.encode_chat_completion(completion_request).tokens\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "input_ids = torch.tensor([tokens]).to(device)\n",
    "\n",
    "# Create attention mask where 1s indicate real tokens and 0s indicate padding\n",
    "attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "# Set the padding tokens to 0\n",
    "# attention_mask[input_ids == tokenizer.pad_token_id] = 0\n",
    "\n",
    "model_inputs = {\n",
    "    \"input_ids\":input_ids,\n",
    "    \"attention_mask\": attention_mask\n",
    "}\n",
    "\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your Hugging Face token as an environment variable\n",
    "os.environ['HF_API_TOKEN'] = 'token_here'\n",
    "\n",
    "# Use the token to login\n",
    "from huggingface_hub import login\n",
    "login(token=os.environ['HF_API_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='cLCLyZsEs', function=Function(arguments='{\"location\": \"Boston, MA\", \"unit\": \"fahrenheit\"}', name='get_current_weather'), type='function')], name=None, tool_call_id=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:8000/v1\", api_key=\"lit\")\n",
    "\n",
    "tools = [\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"get_current_weather\",\n",
    "      \"description\": \"Get the current weather in a given location\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "          },\n",
    "          \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "      },\n",
    "    }\n",
    "  }\n",
    "]\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston today?\"}]\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"Mistral-7B-Instruct-v0.3\",\n",
    "  messages=messages,\n",
    "  tools=tools,\n",
    "  tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None, name=None, tool_call_id=None)\n",
      "ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='JCxndsay5', function=ChoiceDeltaToolCallFunction(arguments='{\"location\": \"Boston, MA\", \"unit\": \"fahrenheit\"}', name='get_current_weather'), type='function')], name=None, tool_call_id=None)\n",
      "ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None, name=None, tool_call_id=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"\",\n",
    "    tools=tools,\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta or \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res message ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='BA0Ri8daj', function=Function(arguments='{\"location\": \"Paris\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')], name=None, tool_call_id=None)\n",
      "ChatCompletion(id='chatcmpl-fd784c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current weather in Paris is 22 degrees Celsius.</s>', role='assistant', function_call=None, tool_calls=None, name=None, tool_call_id=None))], created=1717953900, model='Mistral-7B-Instruct-v0.3', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=0, prompt_tokens=0, total_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:8000/v1\", api_key=\"lit\")\n",
    "\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps(\n",
    "            {\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit}\n",
    "        )\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "\n",
    "SYS_PROMPT = \"You are an advanced language model with the capability to utilize various tools to enhance your responses. Your objective is to provide accurate, relevant, and concise answers to user queries. Use available tools only when necessary to answer the query. If tools are required, return results specifically related to the tools. Otherwise, respond directly without additional information. Before responding, assess whether the query can be answered using your internal knowledge, and only utilize external tools if the query cannot be sufficiently addressed otherwise. When a tool is required, use it effectively and ensure the output is relevant to the query, presenting the tool-generated results directly without additional commentary. The way you use the tools is by specifying a json blob.\\nProvide clear, concise, and direct answers to user queries, ensuring relevance and avoiding unnecessary elaboration. Prioritize user safety and ethical guidelines in all responses, avoiding results that could be harmful, unsafe, or unethical. Carefully consider the potential impact of your response before generating it, and if uncertain about the safety or appropriateness of an answer, err on the side of caution.\\n\\n\"\n",
    "\n",
    "\n",
    "def run_conversation():\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather like in Paris?\",\n",
    "        }\n",
    "    ]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Mistral-7B-Instruct-v0.3\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(**function_args)\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=\"Mistral-7B-Instruct-v0.3\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from the model where it can see the function response\n",
    "        return second_response\n",
    "\n",
    "\n",
    "print(run_conversation())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
